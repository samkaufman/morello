# Seeds for failure cases proptest has generated in the past. It is
# automatically read and these particular cases re-run before any
# novel cases are generated.
#
# It is recommended to check this file in to source control so that
# everyone who runs the test benefits from these saved cases.
cc 36884ecc19b4e544027bd2c09e9f97ad40ddac281ceb7a9265345b9d2c08a5a6 # shrinks to spec = Spec(Compose { components: [PrimitiveBasics { typ: Move, spec_shape: [7, 1, 8], dtypes: [Sint32, Bfloat16] }, PrimitiveBasics { typ: Move, spec_shape: [7, 1, 8], dtypes: [Sint32, Sint32] }, PrimitiveBasics { typ: Max { dim: 1, accum: false }, spec_shape: [7, 1, 8], dtypes: [Float32, Sint32] }], operand_auxes: [TensorSpecAux { contig: 0, aligned: false, level: RF, layout: Layout([(0, Dynamic), (1, Dynamic), (2, Dynamic)]), vector_size: None }, TensorSpecAux { contig: 0, aligned: false, level: VRF, layout: Layout([(0, Dynamic), (1, Dynamic), (2, Dynamic)]), vector_size: Some(8) }], serial_only: false }, Standard(MemVec([2, 11, 0, 0])))
cc f3a9623b6ae4cc98fa1cebc607329abfaae9cdca2b15cf04a9188a9af929c829 # shrinks to spec = Spec(Primitive(PrimitiveBasics { typ: SoftmaxDenominatorAndUnscaled { scan_dim: 2, accum: false }, spec_shape: [1, 1, 4, 1], dtypes: [Bfloat16, Sint32, Uint16] }, [TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (2, Dynamic), (1, Dynamic), (3, Dynamic), (2, Packed(2))], contig: 5 }, vector_size: None }, TensorSpecAux { aligned: true, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 4 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 4 }, vector_size: None }], false), Standard(MemVec([3, 2, 14, 0])))
cc 64f39f2aa65acb6e7de93c873a59b6f2bc153ad281fc01dac8c137bb401025e9 # shrinks to spec = Spec(Primitive(PrimitiveBasics { typ: DivideVecScalar { scan_dim: 2 }, spec_shape: [1, 5, 5, 6], dtypes: [Uint16, Float32, Sint32] }, [TensorSpecAux { contig: 0, aligned: false, level: L1, layout: Layout([(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)]), vector_size: None }, TensorSpecAux { contig: 0, aligned: false, level: L1, layout: Layout([(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)]), vector_size: None }, TensorSpecAux { contig: 0, aligned: false, level: RF, layout: Layout([(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)]), vector_size: None }], true), Standard(MemVec([8, 8, 256*, 0*])))
cc 05436551e30619ca13a599d33260acb607c5bf966e1d93a73292d91185ebea6b # shrinks to spec = Spec(Compose { components: [PrimitiveBasics { typ: SoftmaxComplete { scan_dim: 0 }, spec_shape: [2, 3, 3, 8], dtypes: [Float32, Uint8, Uint8, Uint16] }, PrimitiveBasics { typ: Conv { accum: false }, spec_shape: [2, 3, 8, 2, 7, 4, 2], dtypes: [Sint32, Uint8, Float32] }, PrimitiveBasics { typ: DivideVecScalar { scan_dim: 2 }, spec_shape: [2, 8, 2, 7], dtypes: [Sint16, Bfloat16, Sint32] }], operand_auxes: [TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: true, level: GL, layout: Layout { dims: [(0, Dynamic), (2, Dynamic), (3, Dynamic), (1, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: true, level: RF, layout: Layout { dims: [(1, Dynamic), (2, Dynamic), (3, Dynamic), (0, Dynamic)], contig: 3 }, vector_size: None }, TensorSpecAux { aligned: true, level: L1, layout: Layout { dims: [(0, Dynamic), (2, Dynamic), (1, Dynamic), (3, Dynamic)], contig: 2 }, vector_size: None }], serial_only: true }, Standard(MemVec([11, 12, 8*, 32768*])))
cc bbdceedc81dd8e3db498a4fe2693fd7d773edc2f2f953fc9f55c1938ce9039bc # shrinks to spec = Spec(Compose { components: [PrimitiveBasics { typ: Move, spec_shape: [1, 5, 8, 3], dtypes: [Uint32, Uint8] }, PrimitiveBasics { typ: Conv { accum: false }, spec_shape: [1, 5, 4, 8, 3, 7, 4], dtypes: [Sint16, Sint32, Uint32] }], operand_auxes: [TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }], serial_only: false }, Standard(MemVec([0, 8, 0*, 0*])))
cc d6ce59456e1faeff76f212f962fb4996d0e55069773c7ab002ca693acc7aa4f0 # shrinks to spec = Spec(Compose { components: [PrimitiveBasics { typ: Broadcast { dim: 1 }, spec_shape: [2, 1, 1, 2], dtypes: [Sint16, Uint8] }, PrimitiveBasics { typ: Conv { accum: false }, spec_shape: [2, 1, 2, 1, 2, 1, 3], dtypes: [Sint8, Uint32, Sint16] }, PrimitiveBasics { typ: SoftmaxDenominator { scan_dim: 2, accum: false }, spec_shape: [2, 2, 3, 4], dtypes: [Sint32, Float32, Sint8] }], operand_auxes: [TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: L1, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }], serial_only: false }, Standard(MemVec([6, 1, 256*, 0*])))
cc 5eab8b84019e4a4241c8ddd2df8eb93c2bcbad292191759fa417f28227b7a5fb # shrinks to spec = Spec(Primitive(PrimitiveBasics { typ: Conv { accum: false }, spec_shape: [1, 1, 1, 1, 2, 4, 8], dtypes: [Float32, Sint8, Uint8] }, [TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 0 }, vector_size: None }, TensorSpecAux { aligned: false, level: GL, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (3, Dynamic)], contig: 4 }, vector_size: None }], true), Standard(MemVec([8, 1, 4*, 131072*])))
cc 63378bb1cb6b78306ddc073499068da39667a03d77161b6aef0b3c1ff0059f86 # shrinks to spec = Spec(Primitive(PrimitiveBasics { typ: Matmul { accum: true }, spec_shape: [3, 7, 4, 5], dtypes: [Sint16, Bfloat16, Float32] }, [TensorSpecAux { aligned: false, level: GL, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic)], contig: 1 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic)], contig: 2 }, vector_size: None }, TensorSpecAux { aligned: false, level: RF, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic)], contig: 2 }, vector_size: None }], true), Standard(MemVec([8, 6, 1*, 134217728*])))
cc f9c30bbd453907544c35e119d97b28c338cf27a27823828ef8d35901230ae89e # shrinks to spec = Spec(Primitive(PrimitiveBasics { typ: Matmul { accum: true }, spec_shape: [4, 2, 3, 6], dtypes: [Float32, Bfloat16, Sint32] }, [TensorSpecAux { aligned: true, level: GL, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (0, Packed(2))], contig: 4 }, vector_size: None }, TensorSpecAux { aligned: true, level: GL, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (0, Packed(2))], contig: 2 }, vector_size: None }, TensorSpecAux { aligned: true, level: GL, layout: Layout { dims: [(0, Dynamic), (1, Dynamic), (2, Dynamic), (0, Packed(2))], contig: 2 }, vector_size: None }], true), Standard(MemVec([6, 6, 2048*, 524288*])))
